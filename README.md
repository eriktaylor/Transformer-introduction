# Transformer
This is an overview of the transformer architecture with emphasis on vision-language applications. I'll include short articles to introduce the topic and mini-projects that span recent research in vision, vision-language, and generative AI. 

## Introduction
The Transformer is a general purpose architecture used for machine learning. It is the backend for state of the art generative AI and foundation models that power many recent applications such as ChatGPT, Google Gemini, Stable Diffusion, and others. This is an overview about how it is used for vision applications starting from basics to current state of the art. 

Each topic below is broken down into short articles that require about 15 minutes to read.

## Short articles on transformers

[Transformer introduced for NLP (2017)](https://github.com/TestLinkOpenSourceTRMS/testlink-code)

Vision transformer (2020)

Vision-language transformer

Multimodal and foundation models

## Mini-projects using generative AI

[Review of AI scientist](https://medium.com/@erikntaylor/review-of-ai-scientist-and-related-2024-papers-by-a-human-scientist-with-help-from-gpt-4o-b53c101943ac)
Topics: research paper summary, idea generation, the AI Scientist 

[Visualizing the AI Scientist](https://medium.com/@erikntaylor/visualizing-the-ai-scientist-2aa820ffe1f6)
Topics: generative AI, text-to-image, image-to-text.

If this list grows substantially, I'll move this into a new repository on generative AI.

